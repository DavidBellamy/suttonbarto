# Exercise 3.3 - Line between agent and environment

**Problem Statement**
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out–say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in–say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of *where* to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

# Solution
The purpose of employing this formalism is to learn a policy that maximizes the agent's return (i.e. the expected sum of discounted rewards). So I argue that there is a reason to prefer one choice of representation of the agent versus environment over another. 

I argue that we should prefer the representation that minimizes the sample complexity of learning the optimal policy. If the optimal policy cannot be learned exactly, we should prefer the representation that enables the closest approximation (trading off with sample complexity) – so a composite preference based on approximation quality and sample complexity. 

Given this objective, we can ask what aspects of a representation might maximize this preference? Of course we require that the states obey the Markov property. It is tempting to think that one should choose the representation that gives the agent the most *control* over its environment, since that control is what will enable the agent to most efficiently optimize its policy. This could be expressed in terms of finding a representation that maximizes the mutual information between $A$ and $(S', R)$. Note that even if the mutual information between $(A, S)$ and $S'$ were maximized, if the agent is clueless about how future states relate to rewards $r$, it would still require a lot of exploration to figure out the reward landscape and therefore which states are desirable and hence which actions to take. This would cause the agent's sample complexity to suffer when compared to a representation that has higher mutual information involving the reward signal $r$. 

In general, this mutual information $I(A; (S',R)|S)$ should be maximal when the states are precise and unambiguously descriptive of the environment. Similarly, when the states and actions are represented such that the transitions have minimal conditional entropy $H(S'|S,A)$. This likely requires defining actions that are also very precise and strongly coupled to changes in the environment's state. In fact, $I(A; (S',R)|S) = H(S',R|S) - H(S',R|S,A)$. The first entropy term is the intinsic volatility in the environment whereas the second is the predictability of the environment after the agent chooses an action. Together, these two terms represent the controllability of the environment based on the current choice of representation.

However, making states and actions more precise is equivalent to increasing the size of the state and action spaces, $\mathcal{S,A}$. This, in turn, increases the parameters of any model that we use to approximate functions (value function, policy, transition function, dynamics model), which suffers from the curse of dimensionality. So finding optimal parameters for these models becomes more difficult. Also, the more granular the actions are, the sparser the reward signal will become, which also impedes learning. 

Therefore, we should prefer representations that maximize the agent's control over its environment while minimizing the curse of dimensionality and the sparsity of rewards (to facilitate credit assignment). This defines a Pareto frontier in representation space where we wish to maximize the mutual information $I((S,A); (S', R))$ while minimizing the cardinality of $\mathcal{A, S}$ and the sparsity of $\{R_i\}_{i=1}^T$. The optimum point along this Pareto frontier should constitute the best choice of representation. 

We can express this mathematically as:

$$\min_{\text{boundary B}} [H(S',R|S,A), H(S',R|S), \log|\mathcal{S}|+ \log|\mathcal{A}| + \text{model bits}, \text{reward sparsity}]$$

There are also practical considerations. For example, if observations or actions occur faster than the algorithm can process, we should move the boundary outwards to a higher level. This depends on the bandwidth of the sensors and actuators that we are using in our problem. The Pareto front also needs to consider latency (due to complexity in the RL algorithm) when that is important.